{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)\n",
    "from torch.utils.data import DataLoader\n",
    "import h5py\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "np.random.seed(0)\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models as torchmodels\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs,trainloader,validationloader,verbose=False,plot=False,save=False,loc='chpt'):\n",
    "    '''\n",
    "    args:\n",
    "    \n",
    "    model: pytorch model\n",
    "    epochs: no of epochs\n",
    "    trainloader: Data loader for train set\n",
    "    validationloader: Data loader for validation set\n",
    "    verbose: False/True : If true will print progress as we train \n",
    "    plot: If true will plot the model's loss, accuracy graphs.\n",
    "    \n",
    "    returns: \n",
    "            list[train_acc,valid_acc,train_loss,valid_loss]\n",
    "    '''\n",
    "    # LOSS FUNCTION\n",
    "    \n",
    "    loss_function = nn.BCELoss(reduction='mean')\n",
    "    \n",
    "    #OPTIMIZER\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters()) \n",
    "    train_loss, valid_loss = [], []\n",
    "    train_acc,valid_acc=[],[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TRAINING \n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        if verbose:\n",
    "            print('epoch',epoch)\n",
    "        ## training part \n",
    "        model.train()\n",
    "        ta=0\n",
    "        correctt=0\n",
    "        correctv=0\n",
    "        losst=0\n",
    "        lossv=0\n",
    "        t_k=0\n",
    "        v_k=0\n",
    "        c=1\n",
    "        nb=math.ceil(train_size/batch_size)\n",
    "        for data, target,idx_map in trainloader:\n",
    "            c+=1\n",
    "            t_k=t_k+1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            imgs=load_batch_img('../data/train_images.hdf5',idx_map.cpu().numpy())\n",
    "                \n",
    "            output = model(imgs,data)\n",
    "            del imgs\n",
    "            torch.cuda.empty_cache()\n",
    "            loss = loss_function(output, target)\n",
    "            loss.backward()\n",
    "            losst=losst+loss.item()\n",
    "            optimizer.step()\n",
    "            #print(output)\n",
    "            #print(data.shape)\n",
    "            acc=(output.argmax(dim=1) == target.argmax(dim=1)).float().sum().item()\n",
    "            correctt =correctt + acc\n",
    "            if verbose:\n",
    "                print('\\r'+'batch_no :'+str(c)+' /'+str(nb)+' acc_pred: '+str(acc/batch_size),end='')\n",
    "        train_acc.append(correctt/train_size)\n",
    "        train_loss.append(loss.item())\n",
    "        if verbose or (epoch-1)%10==0:\n",
    "            print('time for epoch',time.time()-start_time)\n",
    "            print('train_loss',losst/t_k)\n",
    "            print('training accuracy',correctt/train_size)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        ## VALIDATION CHECK  \n",
    "        model.eval()\n",
    "        h=0\n",
    "\n",
    "        for data, target,idx_map  in validationloader:\n",
    "            v_k=v_k+1\n",
    "            h+=1\n",
    "            imgs=load_batch_img('../data/train_images.hdf5',idx_map.cpu().numpy())\n",
    "                \n",
    "            output = model(imgs,data)\n",
    "            del imgs\n",
    "            torch.cuda.empty_cache()\n",
    "            loss = loss_function(output, target)\n",
    "            lossv=lossv+loss.item()\n",
    "            accv=(output.argmax(dim=1) == target.argmax(dim=1)).float().sum().item()\n",
    "            correctv =correctv + accv\n",
    "            if verbose:\n",
    "                print('\\r'+'batch_no'+str(h)+' val_acc_pred: '+str(accv/batch_size),end='')\n",
    "        valid_loss.append(lossv/v_k+1)\n",
    "        valid_acc.append(correctv/val_size)\n",
    "        \n",
    "        if verbose or (epoch-1)%10==0:\n",
    "            print('valid_loss',lossv/v_k+1)\n",
    "            print('validation accuracy',correctv/val_size)\n",
    "            \n",
    "            \n",
    "    # SAVING CHECKPOINT\n",
    "    \n",
    "    if save:\n",
    "        torch.save({\n",
    "                'epoch': epochs,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'model':model\n",
    "                }, loc)\n",
    "    \n",
    "    print('train_loss',losst/t_k)\n",
    "    print('valid_loss',lossv/v_k+1)\n",
    "    print('training accuracy',correctt/train_size)\n",
    "    print('validation accuracy',correctv/val_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PLOTTING \n",
    "    if plot:\n",
    "        epc=np.arange(1,epochs+1)\n",
    "        plt.plot(epc,train_acc)\n",
    "        plt.plot(epc,valid_acc)\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(epc,train_loss)\n",
    "        plt.plot(epc,valid_loss)\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "    return train_acc,valid_acc,train_loss,valid_loss,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,x_test_t,y_test,x_idx,disp=False):\n",
    "    '''\n",
    "    args:\n",
    "    \n",
    "    model: pytorch model\n",
    "    x_test: test  set features\n",
    "    y_test: test set labels\n",
    "    \n",
    "    returns:\n",
    "            -\n",
    "    prints:\n",
    "         accuracy\n",
    "         precision\n",
    "         recall\n",
    "         f-score\n",
    "         confusion matrix\n",
    "\n",
    "    '''\n",
    "    out=np.zeros((y_test.shape[0],2))\n",
    "    ind=0\n",
    "    \n",
    "    y_test2=np.zeros((y_test.shape[0],2))\n",
    "    \n",
    "    for i in range(y_test.shape[0]):\n",
    "        if y_test[i]:\n",
    "            y_test2[i][1]=1\n",
    "\n",
    "        else:\n",
    "            y_test2[i][0]=1\n",
    "    vocab_dict=np.load('../data/vocabDict.npy',allow_pickle=True).item()\n",
    "    key_list = list(vocab_dict.keys()) \n",
    "    val_list = list(vocab_dict.values())\n",
    "    y_test=y_test2\n",
    "    \n",
    "\n",
    "    x_vl_t = torch.tensor(x_test_t, dtype=torch.long,device=device)\n",
    "    y_vl = torch.tensor(y_test, dtype=torch.float32,device=device)\n",
    "    x_t_idx=torch.tensor(x_idx.astype('int32'),dtype=torch.long,device=device)\n",
    "    testt = TensorDataset(x_vl_t, y_vl,x_t_idx)\n",
    "    testloader = DataLoader(testt, batch_size=32)\n",
    "    \n",
    "    model.eval()\n",
    "    v_k=0\n",
    "    lossv=0\n",
    "    correctv=0\n",
    "    y_out=[]\n",
    "    h=0\n",
    "    for data, target,idx_map in testloader:\n",
    "        h+=1\n",
    "        ind_l=ind+target.shape[0]\n",
    "        v_k=v_k+1\n",
    "\n",
    "        imgs=load_batch_img('../data/val_images.hdf5',idx_map.cpu().numpy())\n",
    "        \n",
    "        if disp:\n",
    "            s=''\n",
    "            for i in data.cpu().numpy()[0]:\n",
    "                if i in val_list:\n",
    "                    s=s+' '+key_list[val_list.index(i)]\n",
    "                else:\n",
    "                    s=s+' '+'UNK'\n",
    "            print(s)\n",
    "            img=imgs.cpu().numpy()[0]\n",
    "            img=np.moveaxis(img, 0, -1)\n",
    "            plt.imshow(img.astype(int))\n",
    "            plt.show()\n",
    "            print(img.max())\n",
    "            if target[0][0]==1:\n",
    "                print('No')\n",
    "            else:\n",
    "                print('Yes')\n",
    "\n",
    "            print('...........\\n')\n",
    "        \n",
    "        \n",
    "        output = model(imgs,data)\n",
    "        \n",
    "        out2=output.cpu()\n",
    "        out2=out2.detach().numpy()\n",
    "        out2=np.squeeze(out2)\n",
    "        #print(out2.shape,'aaa')\n",
    "        #print(out[ind:ind_l].shape,'aaaaa')\n",
    "        #print(ind,ind_l)\n",
    "        out[ind:ind_l]=out2\n",
    "        ind=ind_l\n",
    "        accv=(output.argmax(dim=1) == target.argmax(dim=1)).float().sum().item()\n",
    "        print('\\r'+'batch_no'+str(h)+' test_acc_pred: '+str(accv/batch_size),end='')\n",
    "        correctv =correctv + accv\n",
    "        del imgs\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print('\\n accuracy')\n",
    "    print(correctv/x_test.shape[0])\n",
    "    y_true=y_test.argmax(axis=1)\n",
    "    y_pred=out.argmax(axis=1)\n",
    "    print('confusion matrix :\\n',confusion_matrix(y_true,y_pred),'\\n')\n",
    "    print('f1 score matrix :\\n',f1_score(y_true,y_pred,average='micro'),'\\n')\n",
    "    print('precision_score :\\n',precision_score(y_true,y_pred,pos_label=1,average='micro'),'\\n')\n",
    "    print('recall_score :\\n',recall_score(y_true,y_pred,pos_label=1,average='micro'),'\\n')\n",
    "    print('classification_report :\\n',classification_report(y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_EXT(vgg):\n",
    "    weights_4096=list(vgg.children())[-1][0].weight\n",
    "    \n",
    "    \n",
    "    return nn.Sequential((*list(vgg.children())[:-1])),weights_4096\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_prams(model):\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_enc(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim,feat_ext_model,\n",
    "                 hidden_size_lstm,weight_4096=None,emb_w=None,emb_Train=False,feat_ext_Train=False,dropout=0.4):\n",
    "        super(base_enc,self).__init__()\n",
    "        \n",
    "        #IMAGE FEATURE EXTRACTOR \n",
    "        self.h_size=hidden_size_lstm\n",
    "        self.feature_extractor = feat_ext_model \n",
    "        \n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = feat_ext_Train\n",
    "        self.conv_linear1=nn.Linear(25088,4096)\n",
    "        if weight_4096 is not None:\n",
    "            self.conv_linear1.weight.data=weight_4096\n",
    "            self.conv_linear1.weight.requires_grad=False\n",
    "\n",
    "        self.conv_linear2=nn.Linear(4096,self.h_size*15)\n",
    "        #TEXT FEATURE EXTRACTOR \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if emb_w is not None:\n",
    "            et = torch.tensor(emb_w, dtype=torch.float32,device=device)\n",
    "            self.embedding.weight = nn.Parameter(et)\n",
    "            self.embedding.weight.requires_grad = emb_Train\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed_size=embedding_dim\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.h_size,dropout=dropout)        \n",
    "        #self.text_linear1= nn.Linear(self.h_size*15, 512)\n",
    "        \n",
    "        # General \n",
    "        self.nn0=nn.Linear(self.h_size*15,512)\n",
    "        self.nn1=nn.Linear(512,128)\n",
    "        self.nn2=nn.Linear(128,2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def img_feat_ext(self,image):\n",
    "        im1= self.feature_extractor(image)\n",
    "        #print(im1.shape)\n",
    "        im1=im1.view(im1.size(0), -1)\n",
    "        im2=self.dropout(self.relu(self.conv_linear1(im1)))\n",
    "        return self.relu(self.dropout(self.conv_linear2(im2)))\n",
    "    \n",
    "    def text_feat_ext(self,text):\n",
    "        h_embedding = self.dropout(self.embedding(text))       \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        conc_h=h_lstm.view(h_lstm.size(0),-1)\n",
    "        \n",
    "        return conc_h\n",
    "    \n",
    "        \n",
    "    def forward(self, image,text):\n",
    "                \n",
    "        image_emb=self.img_feat_ext(image)\n",
    "        text_emb=self.text_feat_ext(text)\n",
    "        \n",
    "        ######## Concatenating feature embeddings\n",
    "        \n",
    "        #print(image_emb.shape)\n",
    "        #joint_emb=torch.cat((image_emb, text_emb), 1)\n",
    "        #print(joint_emb.shape)\n",
    "        \n",
    "        ###### addding embeddings\n",
    "        joint_emb=image_emb*text_emb\n",
    "        out1=self.dropout(self.relu(self.nn0(joint_emb)))\n",
    "        out2=self.dropout(self.relu(self.nn1(out1)))\n",
    "        out3=self.nn2(out2)\n",
    "    \n",
    "      \n",
    "        return self.softmax(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA INTO MEMORY\n",
    "\n",
    "if we have numpy arrays of training images, training text, and class labels it will be easier to load.\n",
    "\n",
    "#### images of shape (no ofinstances ,3,224,224)\n",
    "\n",
    "#### text of shape ( no of instances ,fixed_text_length_n).  ie(no_of_instances,(idx_word1,idx_word2,............,idx_word_n))\n",
    "\n",
    "#### labels of shape (no of instances, class idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(X_train_images,batch_no,batch_size=64): #batch number starting from 0,1,2,3.. batch size default = 64\n",
    "    data={}\n",
    "    start_in= batch_no*batch_size\n",
    "    end_in= (batch_no+1)*batch_size\n",
    "    #print (start_in, end_in)\n",
    "    count = 0\n",
    "    for i in X_train_images:\n",
    "        if ((count >=start_in) and (count <end_in)): \n",
    "            data.update({i: X_train_images[i]}) \n",
    "        count +=1;\n",
    "    return data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_img(file_loc,idx_map):\n",
    "    imgs=np.zeros((idx_map.shape[0],3,224,224))\n",
    "    with h5py.File(file_loc, 'r') as f:\n",
    "        data = f['images']\n",
    "        for i in range(idx_map.shape[0]):\n",
    "            imgs[i]=data[idx_map[i]]\n",
    "    return torch.tensor(imgs,dtype=torch.float,device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_images=np.load('../data/X_train_img.npy')\n",
    "X_train_text=np.load('../data/Question_train_pad.npy')\n",
    "y_train=np.load('../data/answers_train.npy')\n",
    "X_train_idx=np.load('../data/question_image_map_train_array.npy')\n",
    "\n",
    "#X_test_images=np.load('../data/X_test_img.npy')\n",
    "\n",
    "\n",
    "#X_val_images=np.load('../data/X_val_img.npy')\n",
    "#X_val_text=np.load('../data/X_val_text.npy')\n",
    "#y_val=np.load('../data/y_val2.npy')\n",
    "#X_val_idx=np.load('../data/val_img_map.npy')\n",
    "\n",
    "\n",
    "#emb_w=np.load('../data/word_embeddings.npy')\n",
    "glove=np.load('../data/glove_embedding.npy')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes=0\n",
    "no=0\n",
    "y_o=np.zeros((y_train.shape[0],2))\n",
    "for i in range(y_train.shape[0]):\n",
    "    if y_train[i]:\n",
    "        y_o[i][1]=1\n",
    "        yes+=1\n",
    "    else:\n",
    "        y_o[i][0]=1\n",
    "        no+=1\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]]),\n",
       " array([False,  True, False, ...,  True, False,  True]),\n",
       " 84615,\n",
       " 82267)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_o,y_train,yes,no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA INTO GPU AND CREATING DATA ITERATORS \n",
    "\n",
    "we can load into GPU batch by batch if encounter gpu size issues, but it will be slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([166882]), torch.Size([166882, 15]), torch.Size([166882, 2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=78\n",
    "x_tr_t = torch.tensor(X_train_text,dtype=torch.long,device=device)\n",
    "y_tr = torch.tensor(y_o,dtype=torch.float,device=device)\n",
    "x_tr_idx=torch.tensor(X_train_idx.astype('int32'),dtype=torch.long,device=device)\n",
    "x_tr_idx.shape,x_tr_t.shape,y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=x_tr_t.shape[0]\n",
    "val_size=int(0.2*train_size)\n",
    "train_size=train_size-val_size\n",
    "dataset = TensorDataset(x_tr_t, y_tr,x_tr_idx)\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "validloader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PRETRAINED VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16 = torchmodels.vgg16_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device=torch.device(device)\n",
    "VGG_16,w_4096=VGG_EXT(VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12281270\n"
     ]
    }
   ],
   "source": [
    "#img=torch.zeros((10,3,224,224))\n",
    "#text=torch.zeros((10,15),dtype=torch.long)\n",
    "#vocab_size, embedding_dim,feat_ext_model,hidden_size_lstm,\n",
    "model_basic=base_enc(10471,300,VGG_16,128,emb_Train=True,weight_4096=w_4096)\n",
    "print(count_parameters(model_basic))\n",
    "#trainable_prams(model_basic)\n",
    "model_basic=model_basic.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no :1713 /1712 acc_pred: 0.33333333333333336time for epoch 2029.6591002941132\n",
      "train_loss 0.6927266952311881\n",
      "training accuracy 0.5171452968405914\n",
      "batch_no428 val_acc_pred: 0.54871794871794873valid_loss 1.691790198750585\n",
      "validation accuracy 0.5195949185043145\n",
      "epoch 2\n",
      "batch_no :1713 /1712 acc_pred: 0.33333333333333336time for epoch 1956.256514787674\n",
      "train_loss 0.6894726350416089\n",
      "training accuracy 0.5333093643731368\n",
      "batch_no428 val_acc_pred: 0.51282051282051286valid_loss 1.6862847661582108\n",
      "validation accuracy 0.53376677852349\n",
      "epoch 3\n",
      "batch_no :1713 /1712 acc_pred: 0.34615384615384615time for epoch 1910.8537726402283\n",
      "train_loss 0.6861932232767065\n",
      "training accuracy 0.5464024088804997\n",
      "batch_no428 val_acc_pred: 0.57435897435897434valid_loss 1.6852356570067806\n",
      "validation accuracy 0.538470757430489\n",
      "epoch 4\n",
      "batch_no :1713 /1712 acc_pred: 0.34615384615384615time for epoch 1970.676749944687\n",
      "train_loss 0.6820170269739405\n",
      "training accuracy 0.5565667460638474\n",
      "batch_no428 val_acc_pred: 0.46153846153846156valid_loss 1.682487410641162\n",
      "validation accuracy 0.5485378715244487\n",
      "epoch 5\n",
      "batch_no :1713 /1712 acc_pred: 0.34615384615384615time for epoch 1910.8385853767395\n",
      "train_loss 0.6770968861251234\n",
      "training accuracy 0.5672029721510644\n",
      "batch_no428 val_acc_pred: 0.47435897435897434valid_loss 1.6823820001054033\n",
      "validation accuracy 0.545691514860978\n",
      "epoch 6\n",
      "batch_no :1713 /1712 acc_pred: 0.35897435897435974time for epoch 1880.5522260665894\n",
      "train_loss 0.6733506148524373\n",
      "training accuracy 0.5765508666277172\n",
      "batch_no428 val_acc_pred: 0.53846153846153847valid_loss 1.6815780704823609\n",
      "validation accuracy 0.5505153403643337\n",
      "epoch 7\n",
      "batch_no :1713 /1712 acc_pred: 0.39743589743589747time for epoch 1879.7214777469635\n",
      "train_loss 0.6666621897961492\n",
      "training accuracy 0.5877413749194793\n",
      "batch_no428 val_acc_pred: 0.52564102564102573valid_loss 1.6820796091701382\n",
      "validation accuracy 0.5529422339405561\n",
      "epoch 8\n",
      "batch_no :1713 /1712 acc_pred: 0.37179487179487186time for epoch 1833.911072254181\n",
      "train_loss 0.660384307606755\n",
      "training accuracy 0.5981828532050993\n",
      "batch_no428 val_acc_pred: 0.48717948717948717valid_loss 1.682106907679656\n",
      "validation accuracy 0.5524928092042186\n",
      "epoch 9\n",
      "batch_no :1713 /1712 acc_pred: 0.43589743589743593time for epoch 1826.3999955654144\n",
      "train_loss 0.6519559546608791\n",
      "training accuracy 0.6103321199047234\n",
      "batch_no428 val_acc_pred: 0.47435897435897434valid_loss 1.6870868167030477\n",
      "validation accuracy 0.5528523489932886\n",
      "epoch 10\n",
      "batch_no :1713 /1712 acc_pred: 0.33333333333333337time for epoch 1757.3494775295258\n",
      "train_loss 0.6445681471203533\n",
      "training accuracy 0.6209533653918176\n",
      "batch_no428 val_acc_pred: 0.51282051282051283valid_loss 1.6892657681046246\n",
      "validation accuracy 0.5522231543624161\n",
      "epoch 11\n",
      "batch_no :117 /1712 acc_pred: 0.6538461538461539"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8d2bd10af56f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_basic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../checkpoints/basic_model3v2.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-31e7643d44a1>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs, trainloader, validationloader, verbose, plot, save, loc)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "_=train_model(model_basic,epochs,trainloader,validloader,plot=True,verbose=True,save=True,loc='../checkpoints/basic_model3v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([166882]), torch.Size([166882, 15]), torch.Size([166882, 2]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type base_enc. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "                'epoch': epochs,\n",
    "                'model_state_dict': model_basic.state_dict(),\n",
    "                #'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'model':model_basic\n",
    "                }, '../checkpoints/basic_modelv3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text=np.load('../data/Question_val_pad.npy')\n",
    "y_test=np.load('../data/answers_val.npy')\n",
    "X_test_idx=np.load('../data/question_image_map_val_array.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no23 test_acc_pred: 0.20512820512820512"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-3e5def3f5377>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_basic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-6ab814e0eb0f>\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(model, x_test_t, y_test, x_idx, disp)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mout2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mout2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mout2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_model(model_basic,X_test_text,y_test,X_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
