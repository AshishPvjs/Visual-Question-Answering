{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)\n",
    "from torch.utils.data import DataLoader\n",
    "import h5py\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "np.random.seed(0)\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models as torchmodels\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs,trainloader,validationloader,verbose=False,plot=False,save=False,loc='chpt'):\n",
    "    '''\n",
    "    args:\n",
    "    \n",
    "    model: pytorch model\n",
    "    epochs: no of epochs\n",
    "    trainloader: Data loader for train set\n",
    "    validationloader: Data loader for validation set\n",
    "    verbose: False/True : If true will print progress as we train \n",
    "    plot: If true will plot the model's loss, accuracy graphs.\n",
    "    \n",
    "    returns: \n",
    "            list[train_acc,valid_acc,train_loss,valid_loss]\n",
    "    '''\n",
    "    # LOSS FUNCTION\n",
    "    \n",
    "    loss_function = nn.BCELoss(reduction='mean')\n",
    "    \n",
    "    #OPTIMIZER\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters()) \n",
    "    train_loss, valid_loss = [], []\n",
    "    train_acc,valid_acc=[],[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TRAINING \n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        if verbose:\n",
    "            print('epoch',epoch)\n",
    "        ## training part \n",
    "        model.train()\n",
    "        ta=0\n",
    "        correctt=0\n",
    "        correctv=0\n",
    "        losst=0\n",
    "        lossv=0\n",
    "        t_k=0\n",
    "        v_k=0\n",
    "        c=1\n",
    "        nb=math.ceil(train_size/batch_size)\n",
    "        for data, target,idx_map in trainloader:\n",
    "            c+=1\n",
    "            t_k=t_k+1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            imgs=load_batch_img('../data/train_images.hdf5',idx_map.cpu().numpy())\n",
    "                \n",
    "            output = model(imgs,data)\n",
    "            del imgs\n",
    "            torch.cuda.empty_cache()\n",
    "            loss = loss_function(output, target)\n",
    "            loss.backward()\n",
    "            losst=losst+loss.item()\n",
    "            optimizer.step()\n",
    "            #print(output)\n",
    "            #print(data.shape)\n",
    "            acc=(output.argmax(dim=1) == target.argmax(dim=1)).float().sum().item()\n",
    "            correctt =correctt + acc\n",
    "            if verbose:\n",
    "                print('\\r'+'batch_no :'+str(c)+' /'+str(nb)+' acc_pred: '+str(acc/batch_size),end='')\n",
    "        train_acc.append(correctt/train_size)\n",
    "        train_loss.append(loss.item())\n",
    "        if verbose or (epoch-1)%10==0:\n",
    "            print('time for epoch',time.time()-start_time)\n",
    "            print('train_loss',losst/t_k)\n",
    "            print('training accuracy',correctt/train_size)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        ## VALIDATION CHECK  \n",
    "        model.eval()\n",
    "        h=0\n",
    "\n",
    "        for data, target,idx_map  in validationloader:\n",
    "            v_k=v_k+1\n",
    "            h+=1\n",
    "            imgs=load_batch_img('../data/train_images.hdf5',idx_map.cpu().numpy())\n",
    "                \n",
    "            output = model(imgs,data)\n",
    "            del imgs\n",
    "            torch.cuda.empty_cache()\n",
    "            loss = loss_function(output, target)\n",
    "            lossv=lossv+loss.item()\n",
    "            accv=(output.argmax(dim=1) == target.argmax(dim=1)).float().sum().item()\n",
    "            correctv =correctv + accv\n",
    "            if verbose:\n",
    "                print('\\r'+'batch_no'+str(h)+' val_acc_pred: '+str(accv/batch_size),end='')\n",
    "        valid_loss.append(lossv/v_k+1)\n",
    "        valid_acc.append(correctv/val_size)\n",
    "        \n",
    "        if verbose or (epoch-1)%10==0:\n",
    "            print('valid_loss',lossv/v_k+1)\n",
    "            print('validation accuracy',correctv/val_size)\n",
    "            \n",
    "            \n",
    "    # SAVING CHECKPOINT\n",
    "    \n",
    "    if save:\n",
    "        torch.save({\n",
    "                'epoch': epochs,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'model':model\n",
    "                }, loc)\n",
    "    \n",
    "    print('train_loss',losst/t_k)\n",
    "    print('valid_loss',lossv/v_k+1)\n",
    "    print('training accuracy',correctt/train_size)\n",
    "    print('validation accuracy',correctv/val_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PLOTTING \n",
    "    if plot:\n",
    "        epc=np.arange(1,epochs+1)\n",
    "        plt.plot(epc,train_acc)\n",
    "        plt.plot(epc,valid_acc)\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(epc,train_loss)\n",
    "        plt.plot(epc,valid_loss)\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "    return train_acc,valid_acc,train_loss,valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,x_test_t,y_test,x_idx,disp=False):\n",
    "    '''\n",
    "    args:\n",
    "    \n",
    "    model: pytorch model\n",
    "    x_test: test  set features\n",
    "    y_test: test set labels\n",
    "    \n",
    "    returns:\n",
    "            -\n",
    "    prints:\n",
    "         accuracy\n",
    "         precision\n",
    "         recall\n",
    "         f-score\n",
    "         confusion matrix\n",
    "\n",
    "    '''\n",
    "    out=np.zeros((y_test.shape[0],2))\n",
    "    ind=0\n",
    "    \n",
    "    y_test2=np.zeros((y_test.shape[0],2))\n",
    "    \n",
    "    for i in range(y_test.shape[0]):\n",
    "        if y_test[i]:\n",
    "            y_test2[i][1]=1\n",
    "\n",
    "        else:\n",
    "            y_test2[i][0]=1\n",
    "    vocab_dict=np.load('../data/vocabDict.npy',allow_pickle=True).item()\n",
    "    key_list = list(vocab_dict.keys()) \n",
    "    val_list = list(vocab_dict.values())\n",
    "    y_test=y_test2\n",
    "    \n",
    "\n",
    "    x_vl_t = torch.tensor(x_test_t, dtype=torch.long,device=device)\n",
    "    y_vl = torch.tensor(y_test, dtype=torch.float32,device=device)\n",
    "    x_t_idx=torch.tensor(x_idx.astype('int32'),dtype=torch.long,device=device)\n",
    "    testt = TensorDataset(x_vl_t, y_vl,x_t_idx)\n",
    "    testloader = DataLoader(testt, batch_size=32)\n",
    "    \n",
    "    model.eval()\n",
    "    v_k=0\n",
    "    lossv=0\n",
    "    correctv=0\n",
    "    y_out=[]\n",
    "    h=0\n",
    "    for data, target,idx_map in testloader:\n",
    "        h+=1\n",
    "        ind_l=ind+target.shape[0]\n",
    "        v_k=v_k+1\n",
    "\n",
    "        imgs=load_batch_img('../data/val_images.hdf5',idx_map.cpu().numpy())\n",
    "        \n",
    "        if disp:\n",
    "            s=''\n",
    "            for i in data.cpu().numpy()[0]:\n",
    "                if i in val_list:\n",
    "                    s=s+' '+key_list[val_list.index(i)]\n",
    "                else:\n",
    "                    s=s+' '+'UNK'\n",
    "            print(s)\n",
    "            img=imgs.cpu().numpy()[0]\n",
    "            img=np.moveaxis(img, 0, -1)\n",
    "            plt.imshow(img.astype(int))\n",
    "            plt.show()\n",
    "            print(img.max())\n",
    "            if target[0][0]==1:\n",
    "                print('No')\n",
    "            else:\n",
    "                print('Yes')\n",
    "\n",
    "            print('...........\\n')\n",
    "        \n",
    "        \n",
    "        output = model(imgs,data)\n",
    "        \n",
    "        out2=output.cpu()\n",
    "        out2=out2.detach().numpy()\n",
    "        out2=np.squeeze(out2)\n",
    "        #print(out2.shape,'aaa')\n",
    "        #print(out[ind:ind_l].shape,'aaaaa')\n",
    "        #print(ind,ind_l)\n",
    "        out[ind:ind_l]=out2\n",
    "        ind=ind_l\n",
    "        accv=(output.argmax(dim=1) == target.argmax(dim=1)).float().sum().item()\n",
    "        print('\\r'+'batch_no  '+str(h)+' test_acc_pred: '+str(accv/32),end='')\n",
    "        correctv =correctv + accv\n",
    "        del imgs\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print('\\n accuracy')\n",
    "    print(correctv/x_test.shape[0])\n",
    "    y_true=y_test.argmax(axis=1)\n",
    "    y_pred=out.argmax(axis=1)\n",
    "    print('confusion matrix :\\n',confusion_matrix(y_true,y_pred),'\\n')\n",
    "    print('f1 score matrix :\\n',f1_score(y_true,y_pred,average='micro'),'\\n')\n",
    "    print('precision_score :\\n',precision_score(y_true,y_pred,pos_label=1,average='micro'),'\\n')\n",
    "    print('recall_score :\\n',recall_score(y_true,y_pred,pos_label=1,average='micro'),'\\n')\n",
    "    print('classification_report :\\n',classification_report(y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_EXT(vgg):\n",
    "    weights_4096=list(vgg.children())[-1][0].weight\n",
    "    \n",
    "    \n",
    "    return nn.Sequential((*list(vgg.children())[:-1])),weights_4096\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_prams(model):\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_enc(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim,feat_ext_model,\n",
    "                 hidden_size_lstm,weight_4096=None,emb_w=None,emb_Train=False,feat_ext_Train=False,dropout=0.4):\n",
    "        super(base_enc,self).__init__()\n",
    "        \n",
    "        #IMAGE FEATURE EXTRACTOR \n",
    "        self.h_size=hidden_size_lstm\n",
    "        self.feature_extractor = feat_ext_model \n",
    "        \n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = feat_ext_Train\n",
    "        self.conv_linear1=nn.Linear(25088,4096)\n",
    "        if weight_4096 is not None:\n",
    "            self.conv_linear1.weight.data=weight_4096\n",
    "            self.conv_linear1.weight.requires_grad=False\n",
    "\n",
    "        self.conv_linear2=nn.Linear(4096,self.h_size)\n",
    "        #TEXT FEATURE EXTRACTOR \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if emb_w is not None:\n",
    "            et = torch.tensor(emb_w, dtype=torch.float32,device=device)\n",
    "            self.embedding.weight = nn.Parameter(et)\n",
    "            self.embedding.weight.requires_grad = emb_Train\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed_size=embedding_dim\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.h_size,dropout=dropout)        \n",
    "        #self.text_linear1= nn.Linear(self.h_size*15, 512)\n",
    "        \n",
    "        # General \n",
    "        self.nn0=nn.Linear(self.h_size,512)\n",
    "        self.nn1=nn.Linear(512,128)\n",
    "        self.nn2=nn.Linear(128,2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def img_feat_ext(self,image):\n",
    "        im1= self.feature_extractor(image)\n",
    "        #print(im1.shape)\n",
    "        im1=im1.view(im1.size(0), -1)\n",
    "        im2=self.dropout(self.relu(self.conv_linear1(im1)))\n",
    "        return self.relu(self.dropout(self.conv_linear2(im2)))\n",
    "    \n",
    "    def text_feat_ext(self,text):\n",
    "        h_embedding = self.dropout(self.embedding(text))       \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        #conc_h=h_lstm.view(h_lstm.size(0),-1)\n",
    "        \n",
    "        return h_lstm\n",
    "    def attention_net(self, lstm_emb, image_emb):\n",
    "        '''\n",
    "        Arguments\n",
    "\n",
    "        lstm_emb : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
    "        image_emb : Image embedding\n",
    "\n",
    "        Returns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
    "          new hidden state.\n",
    "\n",
    "        '''\n",
    "        #print(lstm_output.shape,final_state.shape)\n",
    "        #print(image_emb.shape,'hidden shape')\n",
    "        #print(image_emb.unsqueeze(2).shape)\n",
    "        attn_weights = torch.bmm(lstm_emb, image_emb.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_emb.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        return new_hidden_state\n",
    "        \n",
    "    def forward(self, image,text):\n",
    "                \n",
    "        image_emb=self.img_feat_ext(image)\n",
    "        text_emb=self.text_feat_ext(text)\n",
    "        \n",
    "        ######## Concatenating feature embeddings\n",
    "        \n",
    "        #print(image_emb.shape)\n",
    "        #joint_emb=torch.cat((image_emb, text_emb), 1)\n",
    "        #print(joint_emb.shape)\n",
    "        \n",
    "        ###### addding embeddings\n",
    "        print(text_emb.shape,image_emb.shape)\n",
    "        att=self.attention_net(text_emb,image_emb)\n",
    "        print(att.shape)\n",
    "        #joint_emb=image_emb*text_emb\n",
    "        out1=self.dropout(self.relu(self.nn0(att)))\n",
    "        out2=self.dropout(self.relu(self.nn1(out1)))\n",
    "        out3=self.nn2(out2)\n",
    "    \n",
    "      \n",
    "        return self.softmax(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA INTO MEMORY\n",
    "\n",
    "if we have numpy arrays of training images, training text, and class labels it will be easier to load.\n",
    "\n",
    "#### images of shape (no ofinstances ,3,224,224)\n",
    "\n",
    "#### text of shape ( no of instances ,fixed_text_length_n).  ie(no_of_instances,(idx_word1,idx_word2,............,idx_word_n))\n",
    "\n",
    "#### labels of shape (no of instances, class idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(X_train_images,batch_no,batch_size=64): #batch number starting from 0,1,2,3.. batch size default = 64\n",
    "    data={}\n",
    "    start_in= batch_no*batch_size\n",
    "    end_in= (batch_no+1)*batch_size\n",
    "    #print (start_in, end_in)\n",
    "    count = 0\n",
    "    for i in X_train_images:\n",
    "        if ((count >=start_in) and (count <end_in)): \n",
    "            data.update({i: X_train_images[i]}) \n",
    "        count +=1;\n",
    "    return data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_img(file_loc,idx_map):\n",
    "    imgs=np.zeros((idx_map.shape[0],3,224,224))\n",
    "    with h5py.File(file_loc, 'r') as f:\n",
    "        data = f['images']\n",
    "        for i in range(idx_map.shape[0]):\n",
    "            imgs[i]=data[idx_map[i]]\n",
    "    return torch.tensor(imgs,dtype=torch.float,device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_images=np.load('../data/X_train_img.npy')\n",
    "X_train_text=np.load('../data/Question_train_pad.npy')\n",
    "y_train=np.load('../data/answers_train.npy')\n",
    "X_train_idx=np.load('../data/question_image_map_train_array.npy')\n",
    "\n",
    "#X_test_images=np.load('../data/X_test_img.npy')\n",
    "\n",
    "\n",
    "#X_val_images=np.load('../data/X_val_img.npy')\n",
    "#X_val_text=np.load('../data/X_val_text.npy')\n",
    "#y_val=np.load('../data/y_val2.npy')\n",
    "#X_val_idx=np.load('../data/val_img_map.npy')\n",
    "\n",
    "\n",
    "#emb_w=np.load('../data/word_embeddings.npy')\n",
    "glove=np.load('../data/glove_embedding.npy')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes=0\n",
    "no=0\n",
    "y_o=np.zeros((y_train.shape[0],2))\n",
    "for i in range(y_train.shape[0]):\n",
    "    if y_train[i]:\n",
    "        y_o[i][1]=1\n",
    "        yes+=1\n",
    "    else:\n",
    "        y_o[i][0]=1\n",
    "        no+=1\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        ...,\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]]),\n",
       " array([False,  True, False, ...,  True, False,  True]),\n",
       " 84615,\n",
       " 82267)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_o,y_train,yes,no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA INTO GPU AND CREATING DATA ITERATORS \n",
    "\n",
    "we can load into GPU batch by batch if encounter gpu size issues, but it will be slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([166882]), torch.Size([166882, 15]), torch.Size([166882, 2]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=64\n",
    "x_tr_t = torch.tensor(X_train_text,dtype=torch.long,device=device)\n",
    "y_tr = torch.tensor(y_o,dtype=torch.float,device=device)\n",
    "x_tr_idx=torch.tensor(X_train_idx.astype('int32'),dtype=torch.long,device=device)\n",
    "x_tr_idx.shape,x_tr_t.shape,y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=x_tr_t.shape[0]\n",
    "val_size=int(0.2*train_size)\n",
    "train_size=train_size-val_size\n",
    "dataset = TensorDataset(x_tr_t, y_tr,x_tr_idx)\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "validloader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PRETRAINED VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16 = torchmodels.vgg16_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device=torch.device(device)\n",
    "VGG_16,w_4096=VGG_EXT(VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4021942\n",
      "torch.Size([1, 15, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 128]) hidden shape\n",
      "torch.Size([1, 128, 1])\n",
      "torch.Size([1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4663, 0.5337]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#img=torch.zeros((1,3,224,224))\n",
    "#text=torch.zeros((1,15),dtype=torch.long)\n",
    "#vocab_size, embedding_dim,feat_ext_model,hidden_size_lstm,\n",
    "model_basic=base_enc(10471,300,VGG_16,256,emb_Train=True,weight_4096=w_4096)\n",
    "print(count_parameters(model_basic))\n",
    "#model_basic.forward(img,text)\n",
    "#trainable_prams(model_basic)\n",
    "model_basic=model_basic.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no :1713 /1712 acc_pred: 0.39743589743589743time for epoch 1875.5054805278778\n",
      "train_loss 0.6919774929775255\n",
      "training accuracy 0.5225907449852442\n",
      "batch_no90 val_acc_pred: 0.51282051282051284"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "_=train_model(model_basic,epochs,trainloader,validloader,plot=True,verbose=True,save=True,loc='../checkpoints/basic_modelv4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
