{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(1)\n",
    "from torch.utils.data import DataLoader\n",
    "import h5py\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models as torchmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,epochs,trainloader,validationloader,verbose=False,plot=False,save=False,loc='chpt'):\n",
    "    '''\n",
    "    args:\n",
    "    \n",
    "    model: pytorch model\n",
    "    epochs: no of epochs\n",
    "    trainloader: Data loader for train set\n",
    "    validationloader: Data loader for validation set\n",
    "    verbose: False/True : If true will print progress as we train \n",
    "    plot: If true will plot the model's loss, accuracy graphs.\n",
    "    \n",
    "    returns: \n",
    "            list[train_acc,valid_acc,train_loss,valid_loss]\n",
    "    '''\n",
    "    # LOSS FUNCTION\n",
    "    \n",
    "    loss_function = nn.BCELoss(reduction='mean')\n",
    "    \n",
    "    #OPTIMIZER\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loss, valid_loss = [], []\n",
    "    train_acc,valid_acc=[],[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TRAINING \n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        if verbose:\n",
    "            print('epoch',epoch)\n",
    "        ## training part \n",
    "        model.train()\n",
    "        ta=0\n",
    "        correctt=0\n",
    "        correctv=0\n",
    "        losst=0\n",
    "        lossv=0\n",
    "        t_k=0\n",
    "        v_k=0\n",
    "        c=1\n",
    "        nb=math.ceil(train_size/batch_size)\n",
    "        for data, target,idx_map in trainloader:\n",
    "            if verbose:\n",
    "                print('\\r'+'batch_no :'+str(c)+' /'+str(nb),end='')\n",
    "            c+=1\n",
    "            t_k=t_k+1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            imgs=load_batch_img('../data/train_images.hdf5',idx_map.cpu().numpy())\n",
    "            \n",
    "            output = model(imgs,data)\n",
    "            del imgs\n",
    "            torch.cuda.empty_cache()\n",
    "            loss = loss_function(output, target)\n",
    "            loss.backward()\n",
    "            losst=losst+loss.item()\n",
    "            optimizer.step()\n",
    "            #print(output)\n",
    "            #print(data.shape)\n",
    "            acc=(output.argmax(dim=1) == target.argmax(dim=1)).float().sum().item()\n",
    "            correctt =correctt + acc\n",
    "        train_acc.append(correctt/train_size)\n",
    "        train_loss.append(loss.item())\n",
    "        if verbose or (epoch-1)%10==0:\n",
    "            print('time for epoch',time.time()-start_time)\n",
    "            print('train_loss',losst/t_k)\n",
    "            print('training accuracy',correctt/train_size)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        ## VALIDATION CHECK  \n",
    "        model.eval()\n",
    "        \n",
    "\n",
    "        for data, target,idx_map  in validationloader:\n",
    "            v_k=v_k+1\n",
    "            \n",
    "            imgs=load_batch_img('../data/train_images.hdf5',idx_map.cpu().numpy())\n",
    "                \n",
    "            output = model(imgs,data)\n",
    "            del imgs\n",
    "            torch.cuda.empty_cache()\n",
    "            loss = loss_function(output, target)\n",
    "            lossv=lossv+loss.item()\n",
    "            accv=(output.argmax(dim=1) == target.argmax(dim=1)).float().sum().item()\n",
    "            correctv =correctv + accv\n",
    "        valid_loss.append(lossv/v_k+1)\n",
    "        valid_acc.append(correctv/val_size)\n",
    "        \n",
    "        if verbose or (epoch-1)%10==0:\n",
    "            print('valid_loss',lossv/v_k+1)\n",
    "            print('validation accuracy',correctv/val_size)\n",
    "            \n",
    "            \n",
    "    # SAVING CHECKPOINT\n",
    "    \n",
    "    if save:\n",
    "        torch.save({\n",
    "                'epoch': epochs,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'model':model\n",
    "                }, loc)\n",
    "    \n",
    "    print('train_loss',losst/t_k)\n",
    "    print('valid_loss',lossv/v_k+1)\n",
    "    print('training accuracy',correctt/train_size)\n",
    "    print('validation accuracy',correctv/val_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #PLOTTING \n",
    "    if plot:\n",
    "        epc=np.arange(1,epochs+1)\n",
    "        plt.plot(epc,train_acc)\n",
    "        plt.plot(epc,valid_acc)\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(epc,train_loss)\n",
    "        plt.plot(epc,valid_loss)\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "    return train_acc,valid_acc,train_loss,valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,x_test_i,x_text_t,y_test):\n",
    "    '''\n",
    "    args:\n",
    "    \n",
    "    model: pytorch model\n",
    "    x_test: test  set features\n",
    "    y_test: test set labels\n",
    "    \n",
    "    returns:\n",
    "            -\n",
    "    prints:\n",
    "         accuracy\n",
    "         precision\n",
    "         recall\n",
    "         f-score\n",
    "         confusion matrix\n",
    "\n",
    "    '''\n",
    "    out=np.zeros((y_test.shape))\n",
    "    ind=0\n",
    "    x_vl = torch.tensor(x_test_i, dtype=torch.long,device=device)\n",
    "    x_vl_t = torch.tensor(x_test_t, dtype=torch.long,device=device)\n",
    "    y_vl = torch.tensor(y_test, dtype=torch.float32,device=device)\n",
    "    testt = TensorDataset(x_vl,x_vl_t, y_vl)\n",
    "    testloader = DataLoader(testt, batch_size=128)\n",
    "    \n",
    "    model.eval()\n",
    "    v_k=0\n",
    "    lossv=0\n",
    "    correctv=0\n",
    "    y_out=[]\n",
    "    for data,data2, target in testloader:\n",
    "        ind_l=ind+target.shape[0]\n",
    "        v_k=v_k+1\n",
    "        output = model(data,data2)\n",
    "        out2=output.cpu()\n",
    "        out2=out2.detach().numpy()\n",
    "        out2=np.squeeze(out2)\n",
    "        #print(out2.shape,'aaa')\n",
    "        #print(out[ind:ind_l].shape,'aaaaa')\n",
    "        #print(ind,ind_l)\n",
    "        out[ind:ind_l]=out2\n",
    "        ind=ind_l\n",
    "        accv=(output.argmax(dim=1) == target.argmax(dim=1)).float().sum().item()\n",
    "        correctv =correctv + accv\n",
    "    print('accuracy')\n",
    "    print(correctv/x_test.shape[0])\n",
    "    y_true=y_test.argmax(axis=1)\n",
    "    y_pred=out.argmax(axis=1)\n",
    "    print('confusion matrix :\\n',confusion_matrix(y_true,y_pred),'\\n')\n",
    "    print('f1 score matrix :\\n',f1_score(y_true,y_pred,average='micro'),'\\n')\n",
    "    print('precision_score :\\n',precision_score(y_true,y_pred,pos_label=1,average='micro'),'\\n')\n",
    "    print('recall_score :\\n',recall_score(y_true,y_pred,pos_label=1,average='micro'),'\\n')\n",
    "    print('classification_report :\\n',classification_report(y_true,y_pred),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_EXT(vgg):\n",
    "    \n",
    "    return nn.Sequential((*list(vgg.children())[:-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_enc(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim,feat_ext_model,\n",
    "                 hidden_size_lstm,emb_w=None,emb_Train=False,feat_ext_Train=False,dropout=0.5):\n",
    "        super(base_enc,self).__init__()\n",
    "        \n",
    "        #IMAGE FEATURE EXTRACTOR \n",
    "        self.feature_extractor = feat_ext_model \n",
    "        \n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = feat_ext_Train\n",
    "        \n",
    "        self.conv_linear1=nn.Linear(25088,4096)\n",
    "        self.conv_linear2=nn.Linear(4096,500)\n",
    "        #TEXT FEATURE EXTRACTOR \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if emb_w is not None:\n",
    "            et = torch.tensor(emb_w, dtype=torch.float32,device=device)\n",
    "            self.embedding.weight = nn.Parameter(et)\n",
    "            self.embedding.weight.requires_grad = emb_Train\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.h_size=hidden_size_lstm\n",
    "        self.embed_size=embedding_dim\n",
    "        self.lstm = nn.LSTM(self.embed_size, self.h_size)        \n",
    "        self.text_linear1= nn.Linear(self.h_size, 500)\n",
    "        \n",
    "        # General \n",
    "        self.nn1=nn.Linear(1000,500)\n",
    "        self.nn2=nn.Linear(500,2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def img_feat_ext(self,image):\n",
    "        im1= self.feature_extractor(image)\n",
    "        #print(im1.shape)\n",
    "        im1=im1.view(im1.size(0), -1)\n",
    "        im2=self.relu(self.conv_linear1(im1))\n",
    "        return self.relu(self.conv_linear2(im2))\n",
    "    \n",
    "    def text_feat_ext(self,text):\n",
    "        h_embedding = self.dropout(self.embedding(text))       \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        last_state, _ = torch.max(h_lstm, 1)   \n",
    "        return self.relu(self.text_linear1(last_state))\n",
    "  \n",
    "        \n",
    "    def forward(self, image,text):\n",
    "                \n",
    "        image_emb=self.img_feat_ext(image)\n",
    "        text_emb=self.text_feat_ext(text)\n",
    "        \n",
    "        ######## Concatenating feature embeddings\n",
    "        \n",
    "        #print(image_emb.shape)\n",
    "        joint_emb=torch.cat((image_emb, text_emb), 1)\n",
    "        #print(joint_emb.shape)\n",
    "        \n",
    "        out1=self.relu(self.nn1(joint_emb))\n",
    "        out2=self.nn2(out1)\n",
    "    \n",
    "      \n",
    "        return self.softmax(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA INTO MEMORY\n",
    "\n",
    "if we have numpy arrays of training images, training text, and class labels it will be easier to load.\n",
    "\n",
    "#### images of shape (no ofinstances ,3,224,224)\n",
    "\n",
    "#### text of shape ( no of instances ,fixed_text_length_n).  ie(no_of_instances,(idx_word1,idx_word2,............,idx_word_n))\n",
    "\n",
    "#### labels of shape (no of instances, class idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(X_train_images,batch_no,batch_size=64): #batch number starting from 0,1,2,3.. batch size default = 64\n",
    "    data={}\n",
    "    start_in= batch_no*batch_size\n",
    "    end_in= (batch_no+1)*batch_size\n",
    "    #print (start_in, end_in)\n",
    "    count = 0\n",
    "    for i in X_train_images:\n",
    "        if ((count >=start_in) and (count <end_in)): \n",
    "            data.update({i: X_train_images[i]}) \n",
    "        count +=1;\n",
    "    return data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_img(file_loc,idx_map):\n",
    "    imgs=np.zeros((idx_map.shape[0],3,224,224))\n",
    "    with h5py.File(file_loc, 'r') as f:\n",
    "        data = f['images']\n",
    "        for i in range(idx_map.shape[0]):\n",
    "            imgs[i]=data[idx_map[i]]\n",
    "    return torch.tensor(imgs,dtype=torch.float,device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_images=np.load('../data/X_train_img.npy')\n",
    "X_train_text=np.load('../data/Question_train_pad.npy')\n",
    "y_train=np.load('../data/answers_train.npy')\n",
    "X_train_idx=np.load('../data/question_image_map_train_array.npy')\n",
    "\n",
    "#X_test_images=np.load('../data/X_test_img.npy')\n",
    "\n",
    "\n",
    "#X_val_images=np.load('../data/X_val_img.npy')\n",
    "#X_val_text=np.load('../data/X_val_text.npy')\n",
    "#y_val=np.load('../data/y_val2.npy')\n",
    "#X_val_idx=np.load('../data/val_img_map.npy')\n",
    "\n",
    "\n",
    "#emb_w=np.load('../data/word_embeddings.npy')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_o=np.zeros((y_train.shape[0],2))\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_o[i][y_train[i]]=1\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA INTO GPU AND CREATING DATA ITERATORS \n",
    "\n",
    "we can load into GPU batch by batch if encounter gpu size issues, but it will be slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([166882]), torch.Size([166882, 15]), torch.Size([166882, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=32\n",
    "x_tr_t = torch.tensor(X_train_text,dtype=torch.long,device=device)\n",
    "y_tr = torch.tensor(y_o,dtype=torch.float,device=device)\n",
    "x_tr_idx=torch.tensor(X_train_idx.astype('int32'),dtype=torch.long,device=device)\n",
    "\n",
    "x_tr_idx.shape,x_tr_t.shape,y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=x_tr_t.shape[0]\n",
    "val_size=int(0.2*train_size)\n",
    "train_size=train_size-val_size\n",
    "dataset = TensorDataset(x_tr_t, y_tr,x_tr_idx)\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "validloader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING PRETRAINED VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16 = torchmodels.vgg16_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(device)\n",
    "VGG_16=VGG_EXT(VGG16)\n",
    "#vocab_size, embedding_dim,feat_ext_model,hidden_size_lstm,\n",
    "model_basic=base_enc(8582,300,VGG_16,128)\n",
    "model_basic=model_basic.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "batch_no :1 /4173"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no :4173 /4173time for epoch 2274.704210281372\n",
      "train_loss 0.693678677939066\n",
      "training accuracy 0.5029436879241382\n",
      "valid_loss 1.6931480498464773\n",
      "validation accuracy 0.07181807286673059\n",
      "epoch 2\n",
      "batch_no :4173 /4173time for epoch 2427.228758096695\n",
      "train_loss 0.6931472022328307\n",
      "training accuracy 0.5018051623148023\n",
      "valid_loss 1.6931471832646618\n",
      "validation accuracy 0.9057106903163951\n",
      "epoch 3\n",
      "batch_no :4173 /4173time for epoch 2134.6922223567963\n",
      "train_loss 0.6931471893777671\n",
      "training accuracy 0.4993034020942879\n",
      "valid_loss 1.6931471824645996\n",
      "validation accuracy 0.39129913710450626\n",
      "epoch 4\n",
      "batch_no :527 /4173"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1fed1fd0d924>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_basic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../checkpoints/basic_model.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-6d711fdb640b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs, trainloader, validationloader, verbose, plot, save, loc)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "_=train_model(model_basic,epochs,trainloader,validloader,plot=True,verbose=True,save=True,loc='../checkpoints/basic_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type base_enc. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "                'epoch': 4,\n",
    "                'model_state_dict': model_basic.state_dict(),\n",
    "                #'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'model':model_basic\n",
    "                }, '../checkpoints/basic_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
